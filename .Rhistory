library(MASS)
library(caret)
# Use scaled data (i.e., correlation matrix basis for PCA)
pressure_vars_scaled <- cleaned_data %>%
dplyr::select(starts_with("V")) %>%
scale()
library(MASS)
library(caret)
library(dplyr)
library(randomForest)
# Step 1: Scale and PCA
pressure_vars_scaled <- scale(dplyr::select(cleaned_data, starts_with("V")))
# Set seed to student number
set.seed(24209758)
# Load the data
pressure_data <- read.csv("Pressure_Data.csv")
# Check the number of rows
nrow(pressure_data)  # Should be 462
# Randomly sample 400 rows
pressure_data_sample <- pressure_data %>% sample_n(400)
#Remove missing values in the factor variables
cleaned_data <- pressure_data_sample %>%
filter(!is.na(Mattress_type) & !is.na(Position) & !is.na(Posture) & !is.na(Subject))
#Extract pressure variables V1 to V144
pressure_vars <- cleaned_data %>% select(starts_with("V"))
#Plot: Boxplots for first 12 variables to check distribution
boxplot(pressure_vars[, 1:12], main = "Boxplot of V1 to V12", las = 2)
#Plot: Histogram of a single variable, e.g., V1
hist(pressure_vars$V1, main = "Histogram of V1", xlab = "Pressure", col = "lightblue", breaks = 20)
#Plot: Correlation heatmap (optional for overall structure)
cor_matrix <- cor(pressure_vars)
heatmap(cor_matrix, main = "Heatmap of Pressure Variable Correlations", symm = TRUE)
#Select and scale pressure data
pressure_vars_scaled <- scale(cleaned_data %>% select(starts_with("V")))
#Hierarchical clustering
dist_matrix <- dist(pressure_vars_scaled)
hclust_result <- hclust(dist_matrix, method = "ward.D2")
#Plot dendrogram
plot(hclust_result, labels = FALSE, main = "Hierarchical Clustering Dendrogram")
rect.hclust(hclust_result, k = 3, border = "red")  # Example with 3 clusters
#K-means clustering (try different k, e.g., 2 to 6)
wss <- sapply(2:6, function(k){
kmeans(pressure_vars_scaled, centers = k, nstart = 20)$tot.withinss
})
plot(2:6, wss, type = "b", main = "Elbow Method", xlab = "Number of clusters", ylab = "WSS")
#Choose k = 3 (say), then apply k-means
kmeans_result <- kmeans(pressure_vars_scaled, centers = 3, nstart = 25)
# Add cluster labels to data
clustered_data <- cleaned_data
clustered_data$HierCluster <- cutree(hclust_result, k = 3)
clustered_data$KMeansCluster <- kmeans_result$cluster
# Step 4: PCA for visualization
pca <- prcomp(pressure_vars_scaled)
plot(pca$x[,1:2], col = clustered_data$KMeansCluster, pch = 19,
main = "K-means Clusters (PCA Projection)", xlab = "PC1", ylab = "PC2")
library(dplyr)
library(MASS)
library(caret)
# Step 1: Prepare data
pressure_vars <- cleaned_data[, grep("^V", names(cleaned_data))]
response <- as.factor(cleaned_data$Posture)
# Step 2: Fit LDA
lda_model <- lda(response ~ ., data = cbind(pressure_vars, response))
lda_pred <- predict(lda_model)
# Step 3: Evaluate performance
conf_matrix <- table(Predicted = lda_pred$class, Actual = response)
print(conf_matrix)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("LDA Classification Accuracy:", round(accuracy, 3), "\n")
library(MASS)
library(caret)
# Use scaled data (i.e., correlation matrix basis for PCA)
pressure_vars_scaled <- cleaned_data %>%
dplyr::select(starts_with("V")) %>%
scale()
subject <- as.factor(cleaned_data$Subject)
# PCA on correlation matrix (scale=TRUE is default when data is already scaled)
pca_result_subject <- prcomp(pressure_vars_scaled)
# Choose enough PCs to explain ~95% variance
expl_var <- summary(pca_result_subject)$importance[3, ]
num_pcs <- which(expl_var >= 0.95)[1]
cat("Number of PCs explaining 95% variance:", num_pcs, "\n")
# Create PCA scores dataframe
pca_scores <- as.data.frame(pca_result_subject$x[, 1:num_pcs])
pca_scores$Subject <- subject
# Train/test split (e.g., 70-30)
set.seed(42)
train_idx <- createDataPartition(pca_scores$Subject, p = 0.7, list = FALSE)
train_data <- pca_scores[train_idx, ]
test_data <- pca_scores[-train_idx, ]
# LDA
lda_model <- lda(Subject ~ ., data = train_data)
lda_pred <- predict(lda_model, test_data)$class
lda_acc <- mean(lda_pred == test_data$Subject)
cat("LDA Accuracy:", round(lda_acc, 3), "\n")
library(MASS)
library(caret)
library(dplyr)
library(randomForest)
# Step 1: Scale and PCA
pressure_vars_scaled <- scale(dplyr::select(cleaned_data, starts_with("V")))
pca_result <- prcomp(pressure_vars_scaled)
# Step 2: Select PCs explaining ~95% variance
expl_var <- summary(pca_result)$importance[3, ]
num_pcs <- which(expl_var >= 0.95)[1]
cat("Using", num_pcs, "PCs\n")
# Step 3: Build dataset
pca_scores <- as.data.frame(pca_result$x[, 1:num_pcs])
pca_scores$Subject <- factor(cleaned_data$Subject)
# Step 4: Cross-validation setup
ctrl <- trainControl(method = "cv", number = 10)
# Step 5: Train LDA
set.seed(123)
lda_model <- train(Subject ~ ., data = pca_scores, method = "lda", trControl = ctrl)
cat("✅ LDA Accuracy (10-fold CV):", round(lda_model$results$Accuracy, 3), "\n")
# Step 6: Train Random Forest (optional)
set.seed(123)
rf_model <- train(Subject ~ ., data = pca_scores, method = "rf", trControl = ctrl)
cat("✅ Random Forest Accuracy (10-fold CV):", round(rf_model$results$Accuracy, 3), "\n")
##*Question8:
Principal Components Regression (PCR) is a technique used to improve regression modelling when the predictors (independent variables) are highly correlated or when there are more predictors than observations. In our case, the 144 pressure variables are likely to be strongly correlated, which can cause multicollinearity, instability in coefficient estimates, and overfitting if regular linear regression is used. PCR helps overcome these issues by reducing the dimensionality of the predictor space before applying regression.
# Load necessary libraries
library(pls)
library(caret)
library(dplyr)
# Step 1: Load Subject_Info and calculate BMI
subject_info <- read.csv("Subject_Info_Data.csv")
# Convert Subject IDs to matching format
cleaned_data$Subject <- gsub("S", "", cleaned_data$Subject)
cleaned_data$Subject <- as.character(cleaned_data$Subject)
subject_info$Subject <- as.character(subject_info$Subject)
# Compute BMI = weight (kg) / height^2 (m^2)
subject_info$BMI <- subject_info$Weight.kg / ((subject_info$Height.cm / 100)^2)
# Step 2: Merge BMI into pressure data
merged_data <- merge(cleaned_data, subject_info[, c("Subject", "BMI")], by = "Subject")
# Step 3: Prepare predictor matrix X and response Y
X <- merged_data %>% dplyr::select(starts_with("V"))
Y <- merged_data$BMI
X_scaled <- scale(X)
# Step 4: Split into training and test sets (70/30)
set.seed(123)
train_idx <- createDataPartition(Y, p = 0.7, list = FALSE)
X_train <- X_scaled[train_idx, ]
X_test <- X_scaled[-train_idx, ]
Y_train <- Y[train_idx]
Y_test <- Y[-train_idx]
# Step 5: Fit Principal Components Regression model using cross-validation
pcr_model <- pcr(Y_train ~ X_train, scale = FALSE, validation = "CV")
# Step 6: Plot CV results and find optimal number of components
validationplot(pcr_model, val.type = "MSEP", main = "PCR Cross-Validation - MSEP")
opt_ncomp <- which.min(pcr_model$validation$PRESS)
cat("✅ Optimal number of components:", opt_ncomp, "\n")
# Step 7: Predict BMI on test set using optimal number of components
Y_pred <- predict(pcr_model, newdata = X_test, ncomp = opt_ncomp)
# Step 8: Evaluate prediction performance (RMSE)
rmse <- sqrt(mean((Y_test - Y_pred)^2))
cat("✅ Test RMSE:", round(rmse, 3), "\n")
# Load necessary libraries
library(pls)
library(caret)
library(dplyr)
# Step 1: Load Subject_Info and calculate BMI
subject_info <- read.csv("Subject_Info_Data.csv")
# Convert Subject IDs to matching format
cleaned_data$Subject <- gsub("S", "", cleaned_data$Subject)
cleaned_data$Subject <- as.character(cleaned_data$Subject)
subject_info$Subject <- as.character(subject_info$Subject)
# Compute BMI = weight (kg) / height^2 (m^2)
subject_info$BMI <- subject_info$Weight.kg / ((subject_info$Height.cm / 100)^2)
# Step 2: Merge BMI into pressure data
merged_data <- merge(cleaned_data, subject_info[, c("Subject", "BMI")], by = "Subject")
# Step 3: Prepare predictor matrix X and response Y
X <- merged_data %>% dplyr::select(starts_with("V"))
Y <- merged_data$BMI
X_scaled <- scale(X)
# Step 4: Split into training and test sets (70/30)
set.seed(123)
train_idx <- createDataPartition(Y, p = 0.7, list = FALSE)
X_train <- X_scaled[train_idx, ]
X_test <- X_scaled[-train_idx, ]
Y_train <- Y[train_idx]
Y_test <- Y[-train_idx]
# Step 5: Fit Principal Components Regression model using cross-validation
pcr_model <- pcr(Y_train ~ X_train, scale = FALSE, validation = "CV")
# Step 6: Plot CV results and find optimal number of components
validationplot(pcr_model, val.type = "MSEP", main = "PCR Cross-Validation - MSEP")
opt_ncomp <- which.min(pcr_model$validation$PRESS)
cat("✅ Optimal number of components:", opt_ncomp, "\n")
# Step 7: Predict BMI on test set using optimal number of components
Y_pred <- predict(pcr_model, newdata = X_test, ncomp = opt_ncomp)
# Step 8: Evaluate prediction performance (RMSE)
rmse <- sqrt(mean((Y_test - Y_pred)^2))
cat("✅ Test RMSE:", round(rmse, 3), "\n")
setwd("D:/MultivariateAnalysis/Assignment")
# Load necessary package
library(dplyr)
source("~/.active-rstudio-document", echo=TRUE)
# Load necessary package
library(dplyr)
suppressPackageStartupMessages({
library(dplyr)
library(caret)
library(MASS)
})
# Set seed to student number
set.seed(24209758)
# Load the data
pressure_data <- read.csv("Pressure_Data.csv")
# Check the number of rows
nrow(pressure_data)  # Should be 462
# Randomly sample 400 rows
pressure_data_sample <- pressure_data %>% sample_n(400)
suppressPackageStartupMessages({
library(dplyr)
library(caret)
library(MASS)
library(ggplot2)
library(RColorBrewer)
library(pls)
})
# Set seed to student number
set.seed(24209758)
# Load the data
pressure_data <- read.csv("Pressure_Data.csv")
# Check the number of rows
nrow(pressure_data)  # Should be 462
# Randomly sample 400 rows
pressure_data_sample <- pressure_data %>% sample_n(400)
#Remove missing values in the factor variables
cleaned_data <- pressure_data_sample %>%
filter(!is.na(Mattress_type) & !is.na(Position) & !is.na(Posture) & !is.na(Subject))
#Extract pressure variables V1 to V144
pressure_vars <- cleaned_data %>% select(starts_with("V"))
#Plot: Boxplots for first 12 variables to check distribution
boxplot(pressure_vars[, 1:12], main = "Boxplot of V1 to V12", las = 2)
#Plot: Histogram of a single variable, e.g., V1
hist(pressure_vars$V1, main = "Histogram of V1", xlab = "Pressure", col = "lightblue", breaks = 20)
#Plot: Correlation heatmap
cor_matrix <- cor(pressure_vars)
heatmap(cor_matrix, main = "Heatmap of Pressure Variable Correlations", symm = TRUE)
#Select and scale pressure data
pressure_vars_scaled <- scale(cleaned_data %>% select(starts_with("V")))
#Hierarchical clustering
dist_matrix <- dist(pressure_vars_scaled)
hclust_result <- hclust(dist_matrix, method = "ward.D2")
#Plot dendrogram
plot(hclust_result, labels = FALSE, main = "Hierarchical Clustering Dendrogram")
rect.hclust(hclust_result, k = 3, border = "red")  # Example with 3 clusters
#K-means clustering (try different k, e.g., 2 to 6)
wss <- sapply(2:6, function(k){
kmeans(pressure_vars_scaled, centers = k, nstart = 20)$tot.withinss
})
plot(2:6, wss, type = "b", main = "Elbow Method", xlab = "Number of clusters", ylab = "WSS")
#Choose k = 3 (say), then apply k-means
kmeans_result <- kmeans(pressure_vars_scaled, centers = 3, nstart = 25)
# Add cluster labels to data
clustered_data <- cleaned_data
clustered_data$HierCluster <- cutree(hclust_result, k = 3)
clustered_data$KMeansCluster <- kmeans_result$cluster
# Step 4: PCA for visualization
pca <- prcomp(pressure_vars_scaled)
plot(pca$x[,1:2], col = clustered_data$KMeansCluster, pch = 19,
main = "K-means Clusters (PCA Projection)", xlab = "PC1", ylab = "PC2")
#Prepare data
pressure_vars <- cleaned_data[, grep("^V", names(cleaned_data))]
response <- as.factor(cleaned_data$Posture)
#Fit LDA
lda_model <- lda(response ~ ., data = cbind(pressure_vars, response))
lda_pred <- predict(lda_model)
#Evaluate performance
conf_matrix <- table(Predicted = lda_pred$class, Actual = response)
print(conf_matrix)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("LDA Classification Accuracy:", round(accuracy, 3), "\n")
#Extract and standardize pressure variables
pressure_vars <- cleaned_data %>% dplyr::select(starts_with("V"))
pressure_scaled <- scale(pressure_vars)
#Run PCA
pca_result <- prcomp(pressure_scaled)
#Plot cumulative variance explained
explained_variance <- summary(pca_result)$importance[3, ]  # cumulative proportion
plot(explained_variance, type = "b", pch = 19,
main = "Cumulative Variance Explained by Principal Components",
xlab = "Number of Principal Components", ylab = "Cumulative Proportion of Variance")
abline(h = 0.95, col = "red", lty = 2)  # 95% threshold
#Get PCA scores (first 2 PCs)
pc_scores <- as.data.frame(pca_result$x[, 1:2])
pc_scores$Posture <- cleaned_data$Posture
#Plot first two principal components colored by posture
ggplot(pc_scores, aes(x = PC1, y = PC2, color = Posture)) +
geom_point(alpha = 0.7, size = 2) +
theme_minimal() +
labs(title = "PCA of Pressure Data (First 2 Components)", x = "PC1", y = "PC2")
#Visualize loadings as heatmaps for interpretation
#For this, reshape PC1 and PC2 loadings into a 16x9 matrix
pc1_loading <- matrix(pca_result$rotation[, 1], nrow = 16, ncol = 9, byrow = TRUE)
pc2_loading <- matrix(pca_result$rotation[, 2], nrow = 16, ncol = 9, byrow = TRUE)
# Optional heatmap visualisation (base R)
par(mfrow = c(1,2))
image(t(apply(pc1_loading, 2, rev)), main = "PC1 Loadings Heatmap", col = heat.colors(100), axes = FALSE)
image(t(apply(pc2_loading, 2, rev)), main = "PC2 Loadings Heatmap", col = heat.colors(100), axes = FALSE)
par(mfrow = c(1,1))
#Get PC1 and PC2 with posture labels
pc_data <- as.data.frame(pca_result$x[, 1:2])
pc_data$Posture <- cleaned_data$Posture
#Fit LDA using PC1 and PC2
lda_pc <- lda(Posture ~ PC1 + PC2, data = pc_data)
#Create a grid for decision boundary
x_min <- min(pc_data$PC1) - 1
x_max <- max(pc_data$PC1) + 1
y_min <- min(pc_data$PC2) - 1
y_max <- max(pc_data$PC2) + 1
grid <- expand.grid(PC1 = seq(x_min, x_max, length.out = 200),
PC2 = seq(y_min, y_max, length.out = 200))
#Predict posture for each grid point
grid$Posture <- predict(lda_pc, newdata = grid)$class
#Plot decision boundaries and points
ggplot() +
geom_tile(data = grid, aes(x = PC1, y = PC2, fill = Posture), alpha = 0.3) +
geom_point(data = pc_data, aes(x = PC1, y = PC2, color = Posture), size = 2) +
labs(title = "LDA Decision Boundaries in PCA Space",
x = "Principal Component 1",
y = "Principal Component 2") +
theme_minimal() +
scale_fill_brewer(palette = "Set2") +
scale_color_brewer(palette = "Dark2")
# Use scaled data (i.e., correlation matrix basis for PCA)
pressure_vars_scaled <- cleaned_data %>%
dplyr::select(starts_with("V")) %>%
scale()
subject <- as.factor(cleaned_data$Subject)
# PCA on correlation matrix (scale=TRUE is default when data is already scaled)
pca_result_subject <- prcomp(pressure_vars_scaled)
# Choose enough PCs to explain ~95% variance
expl_var <- summary(pca_result_subject)$importance[3, ]
num_pcs <- which(expl_var >= 0.95)[1]
cat("Number of PCs explaining 95% variance:", num_pcs, "\n")
# Create PCA scores dataframe
pca_scores <- as.data.frame(pca_result_subject$x[, 1:num_pcs])
pca_scores$Subject <- subject
# Train/test split (e.g., 70-30)
set.seed(42)
train_idx <- createDataPartition(pca_scores$Subject, p = 0.7, list = FALSE)
train_data <- pca_scores[train_idx, ]
test_data <- pca_scores[-train_idx, ]
# LDA
lda_model <- lda(Subject ~ ., data = train_data)
lda_pred <- predict(lda_model, test_data)$class
lda_acc <- mean(lda_pred == test_data$Subject)
cat("LDA Accuracy:", round(lda_acc, 3), "\n")
# Check count per Subject in training set
table(train_data$Subject)
#Load Subject_Info and calculate BMI
subject_info <- read.csv("Subject_Info_Data.csv")
#Convert Subject IDs to matching format
cleaned_data$Subject <- gsub("S", "", cleaned_data$Subject)
cleaned_data$Subject <- as.character(cleaned_data$Subject)
subject_info$Subject <- as.character(subject_info$Subject)
#Compute BMI = weight (kg) / height^2 (m^2)
subject_info$BMI <- subject_info$Weight.kg / ((subject_info$Height.cm / 100)^2)
#Merge BMI into pressure data
merged_data <- merge(cleaned_data, subject_info[, c("Subject", "BMI")], by = "Subject")
#Prepare predictor matrix X and response Y
X <- merged_data %>% dplyr::select(starts_with("V"))
Y <- merged_data$BMI
X_scaled <- scale(X)
#Split into training and test sets (70/30)
set.seed(123)
train_idx <- createDataPartition(Y, p = 0.7, list = FALSE)
X_train <- X_scaled[train_idx, ]
X_test <- X_scaled[-train_idx, ]
Y_train <- Y[train_idx]
Y_test <- Y[-train_idx]
#Fit Principal Components Regression model using cross-validation
pcr_model <- pcr(Y_train ~ X_train, scale = FALSE, validation = "CV")
#Plot CV results and find optimal number of components
validationplot(pcr_model, val.type = "MSEP", main = "PCR Cross-Validation - MSEP")
opt_ncomp <- which.min(pcr_model$validation$PRESS)
cat("Optimal number of components:", opt_ncomp, "\n")
#Predict BMI on test set using optimal number of components
Y_pred <- predict(pcr_model, newdata = X_test, ncomp = opt_ncomp)
#Evaluate prediction performance (RMSE)
rmse <- sqrt(mean((Y_test - Y_pred)^2))
cat("✅ Test RMSE:", round(rmse, 3), "\n")
#Remove missing values in the factor variables
cleaned_data <- pressure_data_sample %>%
filter(!is.na(Mattress_type) & !is.na(Position) & !is.na(Posture) & !is.na(Subject))
#Extract pressure variables V1 to V144
pressure_vars <- cleaned_data %>% select(starts_with("V"))
#Plot: Boxplots for first 12 variables to check distribution
boxplot(pressure_vars[, 1:12], main = "Boxplot of V1 to V12", las = 2)
#Plot: Histogram of a single variable, e.g., V1
hist(pressure_vars$V1, main = "Histogram of V1", xlab = "Pressure", col = "lightblue", breaks = 20)
#Plot: Correlation heatmap
cor_matrix <- cor(pressure_vars)
heatmap(cor_matrix, main = "Heatmap of Pressure Variable Correlations", symm = TRUE)
suppressPackageStartupMessages({
library(dplyr)
library(caret)
library(MASS)
library(ggplot2)
library(RColorBrewer)
library(pls)
})
# Set seed to student number
set.seed(24209758)
# Load the data
pressure_data <- read.csv("Pressure_Data.csv")
# Check the number of rows
nrow(pressure_data)  # Should be 462
# Randomly sample 400 rows
pressure_data_sample <- pressure_data %>% sample_n(400)
#Remove missing values in the factor variables
cleaned_data <- pressure_data_sample %>%
filter(!is.na(Mattress_type) & !is.na(Position) & !is.na(Posture) & !is.na(Subject))
#Extract pressure variables V1 to V144
pressure_vars <- cleaned_data %>% select(starts_with("V"))
#Plot: Boxplots for first 12 variables to check distribution
boxplot(pressure_vars[, 1:12], main = "Boxplot of V1 to V12", las = 2)
#Plot: Histogram of a single variable, e.g., V1
hist(pressure_vars$V1, main = "Histogram of V1", xlab = "Pressure", col = "lightblue", breaks = 20)
#Plot: Correlation heatmap
cor_matrix <- cor(pressure_vars)
heatmap(cor_matrix, main = "Heatmap of Pressure Variable Correlations", symm = TRUE)
#Remove missing values in the factor variables
cleaned_data <- pressure_data_sample %>%filter(!is.na(Mattress_type) & !is.na(Position) & !is.na(Posture) & !is.na(Subject))
#Extract pressure variables V1 to V144
#pressure_vars <- cleaned_data %>% select(starts_with("V"))
pressure_vars <- dplyr::select(cleaned_data, starts_with("V"))
#Plot: Boxplots for first 12 variables to check distribution
boxplot(pressure_vars[, 1:12], main = "Boxplot of V1 to V12", las = 2)
#Plot: Histogram of a single variable, e.g., V1
hist(pressure_vars$V1, main = "Histogram of V1", xlab = "Pressure", col = "lightblue", breaks = 20)
#Plot: Correlation heatmap
cor_matrix <- cor(pressure_vars)
heatmap(cor_matrix, main = "Heatmap of Pressure Variable Correlations", symm = TRUE)
#Remove missing values in the factor variables
cleaned_data <- pressure_data_sample %>%filter(!is.na(Mattress_type) & !is.na(Position) & !is.na(Posture) & !is.na(Subject))
#Extract pressure variables V1 to V144
pressure_vars <- cleaned_data %>% select(starts_with("V"))
#Plot: Boxplots for first 12 variables to check distribution
boxplot(pressure_vars[, 1:12], main = "Boxplot of V1 to V12", las = 2)
#Plot: Histogram of a single variable, e.g., V1
hist(pressure_vars$V1, main = "Histogram of V1", xlab = "Pressure", col = "lightblue", breaks = 20)
#Plot: Correlation heatmap
cor_matrix <- cor(pressure_vars)
heatmap(cor_matrix, main = "Heatmap of Pressure Variable Correlations", symm = TRUE)
library(dplyr)
library(caret)
library(MASS)
#Remove missing values in the factor variables
cleaned_data <- pressure_data_sample %>%filter(!is.na(Mattress_type) & !is.na(Position) & !is.na(Posture) & !is.na(Subject))
#Extract pressure variables V1 to V144
pressure_vars <- cleaned_data %>% select(starts_with("V"))
#Plot: Boxplots for first 12 variables to check distribution
boxplot(pressure_vars[, 1:12], main = "Boxplot of V1 to V12", las = 2)
#Plot: Histogram of a single variable, e.g., V1
hist(pressure_vars$V1, main = "Histogram of V1", xlab = "Pressure", col = "lightblue", breaks = 20)
#Plot: Correlation heatmap
cor_matrix <- cor(pressure_vars)
heatmap(cor_matrix, main = "Heatmap of Pressure Variable Correlations", symm = TRUE)
# Load necessary package
library(dplyr)
